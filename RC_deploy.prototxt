name: "RC-Net"
input: "data"
input_dim: 1
input_dim: 1
input_dim: 256
input_dim: 256

layer {
  name: "conv115"
  type: "Convolution"
  bottom: "data"
  top: "conv115"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 128
    kernel_size: 7
    stride: 1
    pad: 3
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
	bottom: "conv115"
	top: "conv115"
	name: "bn115"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv115"
	top: "conv115"
	name: "scale_conv115"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "prelu115"
  type: "PReLU"
  bottom: "conv115"
  top: "conv115"
}

layer {
  name: "conv116"
  type: "Convolution"
  bottom: "conv115"
  top: "conv116"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 128
    kernel_size: 7
    stride: 1
    pad: 3
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
	bottom: "conv116"
	top: "conv116"
	name: "bn116"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv116"
	top: "conv116"
	name: "scale_conv116"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "prelu116"
  type: "PReLU"
  bottom: "conv116"
  top: "conv116"
}


layer {
  name: "conv200"
  type: "Convolution"
  bottom: "conv116"
  top: "conv200"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
	bottom: "conv200"
	top: "conv200"
	name: "bn200"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv200"
	top: "conv200"
	name: "scale_conv200"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "prelu200"
  type: "PReLU"
  bottom: "conv200"
  top: "conv200"
}


layer {
  name: "conv211"
  type: "Convolution"
  bottom: "conv200"
  top: "conv211"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
	bottom: "conv211"
	top: "conv211"
	name: "bn211"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv211"
	top: "conv211"
	name: "scale_conv211"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "prelu211"
  type: "PReLU"
  bottom: "conv211"
  top: "conv211"
}

layer {
  name: "conv212"
  type: "Convolution"
  bottom: "conv211"
  top: "conv212"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 7
    stride: 1
    pad: 3
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
	bottom: "conv212"
	top: "conv212"
	name: "bn212"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv212"
	top: "conv212"
	name: "scale_conv212"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "prelu212"
  type: "PReLU"
  bottom: "conv212"
  top: "conv212"
}

layer {
  name: "conv213"
  type: "Convolution"
  bottom: "conv212"
  top: "conv213"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
	bottom: "conv213"
	top: "conv213"
	name: "bn213"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv213"
	top: "conv213"
	name: "scale_conv213"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}

layer {
  name: "prelu213"
  type: "PReLU"
  bottom: "conv213"
  top: "conv213"
}

layer {
  name: "conv214"
  type: "Convolution"
  bottom: "conv213"
  top: "conv214"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
	bottom: "conv214"
	top: "conv214"
	name: "bn214"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv214"
	top: "conv214"
	name: "scale_conv214"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "prelu214"
  type: "PReLU"
  bottom: "conv214"
  top: "conv214"
}

layer {
	name: "sum1"
	type: "Eltwise"
    bottom: "conv200"
    bottom: "conv212"
    bottom: "conv214"
	top: "sum1"
	eltwise_param {
	  operation: 1
	}
}




layer {
  name: "conv311"
  type: "Convolution"
  bottom: "sum1"
  top: "conv311"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
	bottom: "conv311"
	top: "conv311"
	name: "bn311"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv311"
	top: "conv311"
	name: "scale_conv311"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "prelu311"
  type: "PReLU"
  bottom: "conv311"
  top: "conv311"
}

layer {
  name: "conv312"
  type: "Convolution"
  bottom: "conv311"
  top: "conv312"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 7
    stride: 1
    pad: 3
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
	bottom: "conv312"
	top: "conv312"
	name: "bn312"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv312"
	top: "conv312"
	name: "scale_conv312"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}

layer {
  name: "prelu312"
  type: "PReLU"
  bottom: "conv312"
  top: "conv312"
}
layer {
  name: "conv313"
  type: "Convolution"
  bottom: "conv312"
  top: "conv313"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
	bottom: "conv313"
	top: "conv313"
	name: "bn313"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv313"
	top: "conv313"
	name: "scale_conv313"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "prelu313"
  type: "PReLU"
  bottom: "conv313"
  top: "conv313"
}

layer {
  name: "conv314"
  type: "Convolution"
  bottom: "conv313"
  top: "conv314"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
	bottom: "conv314"
	top: "conv314"
	name: "bn314"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv314"
	top: "conv314"
	name: "scale_conv314"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}

layer {
  name: "prelu314"
  type: "PReLU"
  bottom: "conv314"
  top: "conv314"
}

layer {
	name: "sum2"
	type: "Eltwise"
    bottom: "sum1"
    bottom: "conv312"
    bottom: "conv314"
    bottom: "conv312"
	top: "sum2"
	eltwise_param {
	  operation: 1
	}
}


layer {
  name: "conv411"
  type: "Convolution"
  bottom: "sum2"
  top: "conv411"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
	bottom: "conv411"
	top: "conv411"
	name: "bn411"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv411"
	top: "conv411"
	name: "scale_conv411"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "prelu411"
  type: "PReLU"
  bottom: "conv411"
  top: "conv411"
}

layer {
  name: "conv412"
  type: "Convolution"
  bottom: "conv411"
  top: "conv412"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 7
    stride: 1
    pad: 3
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
	bottom: "conv412"
	top: "conv412"
	name: "bn412"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv412"
	top: "conv412"
	name: "scale_conv412"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}

layer {
  name: "prelu412"
  type: "PReLU"
  bottom: "conv412"
  top: "conv412"
}
layer {
  name: "conv413"
  type: "Convolution"
  bottom: "conv412"
  top: "conv413"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
	bottom: "conv413"
	top: "conv413"
	name: "bn413"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv413"
	top: "conv413"
	name: "scale_conv413"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}

layer {
  name: "prelu413"
  type: "PReLU"
  bottom: "conv413"
  top: "conv413"
}

layer {
  name: "conv414"
  type: "Convolution"
  bottom: "conv413"
  top: "conv414"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
	bottom: "conv414"
	top: "conv414"
	name: "bn414"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv414"
	top: "conv414"
	name: "scale_conv414"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}

layer {
  name: "prelu414"
  type: "PReLU"
  bottom: "conv414"
  top: "conv414"
}

layer {
	name: "sum3"
	type: "Eltwise"
    bottom: "sum2"
    bottom: "conv412"
    bottom: "conv414"
    bottom: "conv412"
	top: "sum3"
	eltwise_param {
	  operation: 1
	}
}




layer {
  name: "conv511"
  type: "Convolution"
  bottom: "sum3"
  top: "conv511"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
	bottom: "conv511"
	top: "conv511"
	name: "bn511"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv511"
	top: "conv511"
	name: "scale_conv511"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}

layer {
  name: "prelu511"
  type: "PReLU"
  bottom: "conv511"
  top: "conv511"
}

layer {
  name: "conv512"
  type: "Convolution"
  bottom: "conv511"
  top: "conv512"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 7
    stride: 1
    pad: 3
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
	bottom: "conv512"
	top: "conv512"
	name: "bn512"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv512"
	top: "conv512"
	name: "scale_conv512"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}

layer {
  name: "prelu512"
  type: "PReLU"
  bottom: "conv512"
  top: "conv512"
}
layer {
  name: "conv513"
  type: "Convolution"
  bottom: "conv512"
  top: "conv513"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
	bottom: "conv513"
	top: "conv513"
	name: "bn513"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv513"
	top: "conv513"
	name: "scale_conv513"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}

layer {
  name: "prelu513"
  type: "PReLU"
  bottom: "conv513"
  top: "conv513"
}

layer {
  name: "conv514"
  type: "Convolution"
  bottom: "conv513"
  top: "conv514"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
	bottom: "conv514"
	top: "conv514"
	name: "bn514"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv514"
	top: "conv514"
	name: "scale_conv514"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}

layer {
  name: "prelu514"
  type: "PReLU"
  bottom: "conv514"
  top: "conv514"
}

layer {
	name: "sum4"
	type: "Eltwise"
    bottom: "sum3"
    bottom: "conv512"
    bottom: "conv514"
    bottom: "conv512"
	top: "sum4"
	eltwise_param {
	  operation: 1
	}
}

layer {
  name: "conv600"
  type: "Convolution"
  bottom: "sum4"
  top: "conv600"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 128
    kernel_size: 1
    stride: 1
    pad: 0
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
	bottom: "conv600"
	top: "conv600"
	name: "bn600"
	type: "BatchNorm"
	batch_norm_param {
		use_global_stats: false
	}
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
  param {
  lr_mult: 0
  }
}
layer {
	bottom: "conv600"
	top: "conv600"
	name: "scale_conv600"
	type: "Scale"
	scale_param {
		bias_term: true
	}
}
layer {
  name: "prelu600"
  type: "PReLU"
  bottom: "conv600"
  top: "conv600"
}



layer {
  name: "deconv"
  type: "Deconvolution"
  bottom: "conv600"
  top: "deconv"
  param {
    lr_mult: 0.1
  }
  param {
    lr_mult: 0.1
  }
  convolution_param {
    num_output: 1
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
	name: "sum5"
	type: "Eltwise"
    bottom: "deconv"
    bottom: "data"
	top: "sum5"
	eltwise_param {
	  operation: 1
	}
}
 

